<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US"><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en-US" /><updated>2022-03-01T22:03:25+07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">workspace</title><subtitle>This is for frogs in the world.</subtitle><author><name>./workspace</name><email>sewoomkim93@gmail.com</email></author><entry><title type="html">iOS 앱 심사상태를 디스코드로 공유하기 🍎</title><link href="http://localhost:4000/5th-post/" rel="alternate" type="text/html" title="iOS 앱 심사상태를 디스코드로 공유하기 🍎" /><published>2022-02-28T00:00:00+07:00</published><updated>2022-02-28T00:00:00+07:00</updated><id>http://localhost:4000/5th-post</id><content type="html" xml:base="http://localhost:4000/5th-post/"><![CDATA[<h1 id="개발중인-앱의-심사-상태-확인">개발중인 앱의 심사 상태 확인</h1>
<p>개발자라면 <code class="language-plaintext highlighter-rouge">Appstore Connect</code> 사이트에 들어가면 현재 앱의 심사 상태를 확인할 수 있다. 하지만 엔지니어링 팀이 아니라면 권한이 없는 경우도 많기 때문에 모든 팀원이 앱의 현재 상태를 알고 있기란 쉽지 않다. 일일이 공유를 해주는 것도 하나의 방법이기는 하지만 이런 것들이 하나둘씩 쌓이기 시작하면 업무량이 계속해서 늘어나기 때문에 미리미리 자동화를 해두면 나중에 눈물의 작업을 하게 되는 것을 방지할 수 있다. 빠르게 오픈소스를 찾아보았더니 다행히도 <code class="language-plaintext highlighter-rouge">Slack</code>으로 심사 상태를 보내주는 툴이 이미 있었다. <br />
오늘은 <a href="https://fernando.kr/ios/2020-11-08-ios-appstore-status-bot/">Fernando</a>님의 코드를 참고하여 <code class="language-plaintext highlighter-rouge">Discord</code>로 앱의 심사 상태를 보내주는 툴을 만들어보자.</p>

<h1 id="사용하는-기능">사용하는 기능</h1>
<p>● <a href="https://developer.apple.com/documentation/appstoreconnectapi">Appstore Connect API</a><br />
● <a href="https://github.com/fastlane/fastlane/tree/master/spaceship">fastlane Spaceship</a><br />
● <a href="https://docs.github.com/en/actions">GitHub Actions</a><br />
● <a href="https://gist.github.com">GitHub Gist</a><br />
● <a href="https://support.discord.com/hc/en-us/articles/228383668-Intro-to-Webhooks">Discord Webhook</a></p>

<h1 id="코드의-흐름">코드의 흐름</h1>
<ol>
  <li>Appstore Connect 사이트에서 API key를 발급받는다.</li>
  <li>Spaceship 라이브러리를 이용하여 앱 심사 정보를 받아오는 코드를 짠다.</li>
  <li>GitHub Actions를 사용하여 일정시간마다 위의 코드를 반복적으로 돌릴 수 있게 한다.</li>
  <li>코드가 실행될 때마다 GitHub Gist에 앱의 현재 정보가 저장된다.</li>
  <li>이전에 등록된 정보와 현재 앱의 정보가 다르면 Webhook을 이용하여 현재 앱의 심사 상태를 Discord로 보낸다.</li>
</ol>

<h1 id="툴의-특징">툴의 특징</h1>
<p>● 설정이 직관적이고 간단<br />
● 개인 서버가 없어도 작동 가능<br />
<br />
<code class="language-plaintext highlighter-rouge">Fernando</code>님의 글에서도 알 수 있듯이 <strong>간편함</strong>이 제일 중요하기에 봇 개발에 리소스를 쓰고 싶지 않은 분들을 위해 레포지토리 <a href="https://github.com/froggydisk/app-status-bot">Fork</a>만으로 작동할 수 있게 하였다.</p>

<h1 id="사용법">사용법</h1>
<ol>
  <li>깃허브 레포지토리를 <a href="https://github.com/froggydisk/app-status-bot">Fork</a>합니다.</li>
  <li>Appstore Connect에서 key ID, issuer ID, bundle ID, API Key file (.p8)을 발급받습니다.</li>
  <li>Discord에서 Webhook url을 발급받습니다.</li>
  <li>GitHub token을 발급받습니다. (발급시 gists와 repo 항목에 체크해주세요.)</li>
  <li>앱 정보를 저장할 Gist의 url에서 맨 뒤의 ID만 저장해둡니다.<br />
e.g.) https://gist.github.com/froggydisk/[이부분]</li>
  <li>위에서 저장한 총 7개의 key를 Fork한 레포지토리의 secret에 하나씩 입력합니다.<br />
<code class="language-plaintext highlighter-rouge">Settings</code> → <code class="language-plaintext highlighter-rouge">Secrets</code> → <code class="language-plaintext highlighter-rouge">Actions</code> → <code class="language-plaintext highlighter-rouge">New repositoy secret</code>
    <blockquote>
      <p>● PRIVATE_KEY: Appstore Connect API Key file (.p8)<br />
● KEY_ID : Appstore Connect key ID<br />
● ISSUER_ID : Appstore Connect issuer ID <br />
● BUNDLE_ID : Appstore Connect bundle ID with comma<br />
● DISCORD_WEBHOOK : Discord Webhook url<br />
● GH_TOKEN: GitHub token<br />
● GIST_ID: Gist url ID</p>

    </blockquote>
  </li>
  <li>마지막으로 레포지토리의 <code class="language-plaintext highlighter-rouge">Actions</code> 섹션으로 가서 workflow를 활성화해주면 끝!</li>
</ol>

<h1 id="주의점">주의점</h1>
<p>일단 discord로 메세지를 보내는 라이브러리는 정말 다양하게 있다! <a href="https://www.npmjs.com">npm</a>에서 마음에 드는 것을 찾아보자. npm으로 다운받은 뒤에는 discord.js의 코드를 사용법에 맞추어 바꿔주어야한다. Slack이나 Discord 이외에 다른 툴을 사용하는 경우에는 참고하자.<br />
<br />
다음은 <code class="language-plaintext highlighter-rouge">Fernando</code>님의 <a href="https://github.com/techinpark/appstore-status-bot">레포지토리</a>를 그대로 사용하다보면 가끔씩 오작동하는 이슈가 있어서 원인을 찾아보았다.</p>
<h4 id="1-js의-비동기-처리">1. js의 비동기 처리</h4>
<p>GitHub Actions에서 fetch.yml을 실행하다보면 우선적으로 Gist의 store.db 정보를 쭉 불러온다. 하지만 store.db 안의 내용이 길어지기 시작하면서 request.get 할 때 시간이 오래걸리기 시작하더니 store.db를 다 읽기 전에 현재 앱 상태와의 비교가 끝나버리는 경우가 있었다. 실제 심사 상태는 변화가 없는데 Discord로 알림이 가버렸다. 해결을 위해 앞에 간단하게 await를 붙여주었다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="n">function</span> <span class="n">getGist</span><span class="p">()</span> <span class="p">{</span>
    <span class="p">(</span><span class="n">생략</span><span class="p">)</span>
    <span class="k">await</span> <span class="n">request</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">options</span><span class="p">,</span> <span class="n">function</span><span class="p">(</span><span class="n">error</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">body</span><span class="p">){</span>
        <span class="p">(</span><span class="n">생략</span><span class="p">)</span>
    <span class="p">});</span>
<span class="p">}</span>
</code></pre></div></div>
<h4 id="2-특정-상태만-체크">2. 특정 상태만 체크</h4>
<p>반대로 상태가 바뀌었는데도 알림이 안오는 경우도 있었다. 해결을 위해 현재 코드에서는 가장 최신 버전에 대해 모든 상태 변화를 체크하도록 수정했는데 혹시나 특정 상태에 대해서만 알림을 보내고 싶다면 <a href="https://github.com/fastlane/fastlane/blob/master/spaceship/docs/AppStoreConnect.md">Spaceship</a>의 문서를 잘 살펴보는 것을 추천한다. 아래는 Spaceship에서 버전을 가져오는 함수들이다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">app</span><span class="p">.</span><span class="n">get_live_app_store_version</span> <span class="c1"># the version that's currently available in the App Store
</span><span class="n">app</span><span class="p">.</span><span class="n">get_edit_app_store_version</span> <span class="c1"># the version that's in `Prepare for Submission`, `Metadata Rejected`, `Rejected`, `Developer Rejcted`, `Waiting for Review`, `Invalid Binary` mode
</span><span class="n">app</span><span class="p">.</span><span class="n">get_latest_app_store_version</span> <span class="c1"># the version that's the latest one
</span><span class="n">app</span><span class="p">.</span><span class="n">get_pending_release_app_store_version</span> <span class="c1"># the version that's in `Pending Developer Release` or `Pending Apple Release` mode
</span><span class="n">app</span><span class="p">.</span><span class="n">get_in_review_app_store_version</span> <span class="c1"># the version that is in `In Review` mode
</span></code></pre></div></div>

<h1 id="결과">결과</h1>
<p><img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/discord-noti.png?raw=true" width="600" /><br />
제 때 알림이 온다!! 다만 GitHub Actions는 계정당 쓸 수 있는 시간이 한계가 있으므로 Cron Job을 너무 자주 돌리진 말자.<br />
<br />
📍 <a href="https://github.com/froggydisk/app-status-bot">코드 보러가기</a></p>]]></content><author><name>./workspace</name><email>sewoomkim93@gmail.com</email></author><category term="Blog" /><category term="API" /><summary type="html"><![CDATA[개발중인 앱의 심사 상태 확인 개발자라면 Appstore Connect 사이트에 들어가면 현재 앱의 심사 상태를 확인할 수 있다. 하지만 엔지니어링 팀이 아니라면 권한이 없는 경우도 많기 때문에 모든 팀원이 앱의 현재 상태를 알고 있기란 쉽지 않다. 일일이 공유를 해주는 것도 하나의 방법이기는 하지만 이런 것들이 하나둘씩 쌓이기 시작하면 업무량이 계속해서 늘어나기 때문에 미리미리 자동화를 해두면 나중에 눈물의 작업을 하게 되는 것을 방지할 수 있다. 빠르게 오픈소스를 찾아보았더니 다행히도 Slack으로 심사 상태를 보내주는 툴이 이미 있었다. 오늘은 Fernando님의 코드를 참고하여 Discord로 앱의 심사 상태를 보내주는 툴을 만들어보자.]]></summary></entry><entry><title type="html">Tensor element를 mutable하게 복사하기</title><link href="http://localhost:4000/4th-post/" rel="alternate" type="text/html" title="Tensor element를 mutable하게 복사하기" /><published>2021-08-29T00:00:00+07:00</published><updated>2021-08-29T00:00:00+07:00</updated><id>http://localhost:4000/4th-post</id><content type="html" xml:base="http://localhost:4000/4th-post/"><![CDATA[<p><br />
일반적인 경우, 텐서 간의 복사는 복사된 참조 변수의 수정이 기존 참조 변수의 값에 똑같은 영향을 미친다.
예를 들면 아래와 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="c1">#tensor([1., 2., 3.])
</span><span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">4</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="c1">#tensor([4., 2., 3.])
</span></code></pre></div></div>

<p>하지만 아래와 같이 a와 b의 길이가 다르고 a의 요소들을 b에 배분하는 형식인 경우, 즉 텐서 요소 간의 복사에 있어서는 복사된 참조 변수의 수정이 기존 참조 변수에 영향을 미치지 못한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">0.</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
  <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">b</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="c1">#tensor([1., 2., 3., 2.])
</span><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="c1">#tensor([1., 2., 3., 2.])
</span></code></pre></div></div>

<p>위와는 조금 다르게, b가 텐서가 아니라 리스트나 numpy배열일 경우에 있어서 b의 각 요소는 아래처럼 mutable하다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
  <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">b</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="c1">#[tensor(1.), tensor(2.), tensor(3.), tensor(2.)]
</span><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="c1">#[tensor(1.), tensor(3.), tensor(3.), tensor(3.)]
</span></code></pre></div></div>

<p>다만 리스트나 넘파이를 사용할 경우에는 위의 예와 같이 리스트 안에 텐서가 여러개 들어가 있는 형태(list of tensors)가 되어버리는데 이를 그대로 torch.tensor(b)와 같이 텐서로 바꿔버리면 grad가 끊기면서 loss.backward()시에</p>
<blockquote>
  <p>RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</p>
</blockquote>

<p>와 같은 에러가 뜨게 된다. 그 때 쓸 수 있는 방법 중 하나가 torch.stack 이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="c1">#tensor([1., 2., 3., 2.], grad_fn=&lt;StackBackward&gt;)
</span></code></pre></div></div>

<p>이걸 쓰면 grad_fn을 유지하면서 list of tensors를 하나의 텐서로 만들어 줄 수 있다. 
주의할 점은 새로운 변수 c에 선언하는게 아닌 b=torch.stack(b)와 같이 b에 덮어씌우게 되면 나중에 파라미터 업데이트 시에 b는 업데이트 되지 않는다. 이번 경우는 a와 b가 동시에 업데이트 되기를 원하므로 위와 같이 c에 새로운 텐서를 만들어주었다.</p>

<p>전체 코드는</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.optim</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span><span class="mf">2.</span><span class="p">,</span><span class="mf">3.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
  <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">b</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">my_loss</span><span class="p">(</span><span class="n">embedding</span><span class="p">):</span> <span class="c1">#contrastive loss
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">((</span><span class="n">embedding</span><span class="p">.</span><span class="n">exp</span><span class="p">()[:</span><span class="mi">1</span><span class="p">].</span><span class="nb">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">embedding</span><span class="p">.</span><span class="n">exp</span><span class="p">().</span><span class="nb">sum</span><span class="p">()).</span><span class="n">log</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">loss</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">([</span><span class="n">a</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">my_loss</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="c1">#[tensor(1.0009, grad_fn=&lt;AsStridedBackward&gt;), tensor(1.9996, ...]
</span><span class="k">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="c1">#tensor([1.0009, 1.9996, 2.9995], requires_grad=True)
</span></code></pre></div></div>
<p>a와 b가 성공적으로 동시에 업데이트 된다. 
물론 단순히 1차원 텐서가 아니라 그 이상의 텐서에 대해서도 적용 가능하다.</p>]]></content><author><name>./workspace</name><email>sewoomkim93@gmail.com</email></author><category term="Blog" /><category term="Pytorch" /><summary type="html"><![CDATA[머신러닝/딥러닝]]></summary></entry><entry><title type="html">모바일에서 hover 효과 없애기</title><link href="http://localhost:4000/3rd-post/" rel="alternate" type="text/html" title="모바일에서 hover 효과 없애기" /><published>2020-08-28T00:00:00+07:00</published><updated>2020-08-28T00:00:00+07:00</updated><id>http://localhost:4000/3rd-post</id><content type="html" xml:base="http://localhost:4000/3rd-post/"><![CDATA[<p><br />
아이폰이나 아이패드와 같은 기기에서는 PC와는 다르게 hover 효과가 적용되지 않는다.<br />
그 이유는 마우스가 아닌 터치 방식을 사용하기 때문이다.<br />
그러다보니 모바일에서 드랍다운 메뉴 같은 경우, 한 번 누르면 계속 호버된 상태가 되어 효과가 사라지지 않는 경우가 있다.<br />
이 때 화면의 다른 부분을 누르면 호버효과가 사라지게 만들고 싶을 때 쓸만한 방법이 있다.</p>
<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">&lt;!DOCTYPE html&gt;</span>
<span class="nt">&lt;html&gt;</span>
  <span class="nt">&lt;head&gt;</span>
    <span class="nt">&lt;meta</span> <span class="na">charset=</span><span class="s">"utf-8"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;meta</span> <span class="na">name=</span><span class="s">"viewport"</span> <span class="na">content=</span><span class="s">"width=device-width"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;title&gt;</span>모바일에서 hover 효과 없애기<span class="nt">&lt;/title&gt;</span>
    <span class="nt">&lt;link</span> <span class="na">href=</span><span class="s">"style.css"</span> <span class="na">rel=</span><span class="s">"stylesheet"</span> <span class="na">type=</span><span class="s">"text/css"</span> <span class="nt">/&gt;</span>
    <span class="nt">&lt;style&gt;</span>
<span class="k">@media</span> <span class="p">(</span> <span class="n">max-width</span><span class="p">:</span> <span class="m">768px</span> <span class="p">){</span>
        <span class="nt">button</span> <span class="p">{</span>
          <span class="nl">-webkit-tap-highlight-color</span><span class="p">:</span><span class="nb">transparent</span><span class="p">;</span>
        <span class="p">}</span>
      <span class="p">}</span>
    <span class="nt">&lt;/style&gt;</span>
  <span class="nt">&lt;/head&gt;</span>
  <span class="nt">&lt;body</span> <span class="na">ontouchstart=</span><span class="s">""</span><span class="nt">&gt;</span>

  <span class="nt">&lt;/body&gt;</span>
</code></pre></div></div>
<p>호버 효과를 적용하는 class의 style 안에 <strong>-webkit-tap-highlight-color:transparent;</strong>를 넣어주고(터치 시 탭효과 제거)<br />
body에 <strong>ontouchstart=”“</strong>를 적용하면 끝</p>]]></content><author><name>./workspace</name><email>sewoomkim93@gmail.com</email></author><category term="Blog" /><category term="HTML/CSS" /><summary type="html"><![CDATA[반응형 웹페이지 만들기]]></summary></entry><entry><title type="html">CAM: Class Activation Map Implementation</title><link href="http://localhost:4000/assignment/" rel="alternate" type="text/html" title="CAM: Class Activation Map Implementation" /><published>2020-08-04T00:00:00+07:00</published><updated>2020-08-04T03:41:00+07:00</updated><id>http://localhost:4000/assignment</id><content type="html" xml:base="http://localhost:4000/assignment/"><![CDATA[<p><br /></p>
<p align="justify">
Today, I'm going to talk about a paper, 'Learning Deep Features for Discriminative Localization'
(<a href="https://arxiv.org/pdf/1512.04150.pdf">Zhou+ CVPR16</a>).
I will review this paper lightly first and try to implement the main functions of CAM with Pytorch, which are introduced in the paper.  
</p>

<h2 id="background">Background</h2>

<p align="justify">
Over a long history of CNN, people made a great effort to increase the accuracy of the trained model. Since 2012, deep learning techniques have developed overwhelmingly 
and have now surpassed human beings in terms of object recognition.
However, what they had known was that filters could find the edge of an object at the shallow layers and capture the high-dimensional features as the network goes deeper.
The important thing is, they didn't know why the machine made such a judgment when it was asked to guess what the object was. 
That is to say, there was not any method that could explain the process of the machine's thinking.  
<br /><br />
<em>CAM</em>, which was developed by Bolei Zhou+ in 2016, is the very way to solve this problem. 
Using CAM, we can interpret an image from the machine's point of view and explain which parts of the image are utilized to make a decision.
</p>

<h2 id="introduction">Introduction</h2>

<p align="justify">
In this paper, the author showed that trained CNN model is successfully able to localize the discriminative regions of an object for classification despite no data on the location
of the object was provided.  
</p>

<p align="justify">
Generally, a CNN model has a Fully-Connected layer(FC) at the last part of the network, which is used to generate the final output. To use the FC layer, however, the features 
need to be flattened while losing the spatial information that convolution layers accumulated through the previous parts of the network.
This is the main reason that makes the model lose the ability to localize objects.  <br />
Recently, popular CNN models such as NIN(Network in Network) and GoogLeNet have been proposed to avoid FC layer not only to keep the localization ability which is mentioned 
above, but also to minimize the number of parameters while maintaining high performance.  <br />
In order to achive this, the author just applied GAP(global average pooling) to the model.
The use of GAP prevented overfitting during training and encouraged the network to identify the complete extent of the object at the same time.
After the application of GAP, trained CNN model actually gets to build a generic localizable deep representation that exposes the implicit attention of CNNs on an image.
</p>

<p align="justify">
What we have to look at is that while GAP is not a novel techinique at all, which is even simple technique with little computational cost, the unique observation that it can be
applied for accurate discriminative localizations offered a new paradigm in ML model analysis.  <br />
This approach should be the core contribution of this paper and it provides us with another glimpse into the soul of CNNs.  
</p>

<h2 id="class-activation-mapcam">Class Activation Map(CAM)</h2>

<p align="justify">
CAM actually works at the end of the network, just before the final output layer(softmax in the case of categorization).
At this point, GAP is applied to the convolutional feature maps and the features after the GAP layer finally pass through the last FC layer.
(This network uses only one FC layer)
And then, CAM identifies the importance of the image regions by projecting back the weights of the output layer onto the convolutional feature maps. 
</p>

<p><strong>To explain the concept with equations,</strong><br />
Let $f_k(x,y)$ represent the activation of unit k in the last convolutional layer at spatial location (x,y). Then, for unit k, the result of performing GAP is expressed
as $F^k$ and it equals to $\sum_{x, y}f_k(x,y)$. Thus for a given class c, the input to the softmax, $S_c$ is $\sum_k w_{k}^{c}F_k$, where $w_{k}^{c}$ is the weight 
corresponding to class c for unit k. Essentially, $w_{k}^{c}$ indicates the importnace of $F_k$ for class c. To sum up, it becomes like this.</p>

<p>$\begin{matrix}
S_c &amp;=&amp; \sum_k w_k^c F_k <br />
&amp;=&amp; \sum_k w_c^k \sum_{x, y}f_k(x,y) <br />
&amp;=&amp; \sum_{x, y} \sum_k w_k^c f_k(x,y) 
\end{matrix}$</p>

<p>If we define $M_c$ as the CAM for class c, where each spatial element is given by $M_c(x, y)$ = $\sum_k w_k^c f_k(x, y)$, we can find $S_c = \sum_{x, y} M_c(x, y)$.
Hence, $M_c(x,y)$ directly indicates the importance of the activation at spatial grid (x,y) leading to the classification of an image to class c.</p>

<p align="justify">
Therefore, the class activation map is simply a weighted linear sum of the presence of visual patterns at different spatial location. By simply upsampling the class activation map 
to the size of the input image, the regions of the image that are most relevant to the particular category can be identified.
</p>

<p><img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/CAM%20structure.png?raw=true" alt="structure" class="align-center" /></p>

<h2 id="implementation">Implementation</h2>

<p align="justify">
I tried to implement the main functions of the original project and visually check their results.
You can refer to the code <a href="https://github.com/froggydisk/CAM">here(CAM Implementation)</a>.
I used Pytorch and you can simply run the code on GoogleColab. 
</p>

<p align="justify">
I made a very simple network first and used CIFAR10 for training.
After the training is finished, the model becomes able to classify an object and we can find the class with the highest probability.
Then, we can draw a heatmap by multiplying the corresponding weights with each feature maps that came out from the last convolutional layer.  
</p>

<p>The code below shows the main parts of the CAM function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_collection</span> <span class="o">=</span> <span class="p">[]</span> 
<span class="c1"># get features from the input
</span><span class="k">def</span> <span class="nf">get_feature</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>                                         
  <span class="n">_</span><span class="p">,</span> <span class="n">feature</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
  <span class="n">feature_collection</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">feature</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="c1"># get weights from the final layer
</span><span class="n">weight_for_softmax</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">].</span><span class="n">cpu</span><span class="p">().</span><span class="n">data</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span>

<span class="c1"># draw a heatmap
</span><span class="k">def</span> <span class="nf">Do_CAM</span><span class="p">(</span><span class="n">feature</span><span class="p">,</span> <span class="n">weigth_for_softmax</span><span class="p">,</span> <span class="n">class_id</span><span class="p">):</span> 
  <span class="n">upsample_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">img_size</span><span class="p">,</span> <span class="n">img_size</span><span class="p">)</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">feature</span><span class="p">.</span><span class="n">shape</span>
  <span class="c1"># (weights) x (feature maps)
</span>  <span class="n">cam</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">weight_for_softmax</span><span class="p">[</span><span class="n">class_id</span><span class="p">],</span><span class="n">feature</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="o">*</span><span class="n">w</span><span class="p">))</span>  
  <span class="n">cam</span> <span class="o">=</span> <span class="n">cam</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
  <span class="n">cam</span> <span class="o">=</span> <span class="p">(</span><span class="n">cam</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">cam</span><span class="p">))</span> 
  <span class="n">cam</span> <span class="o">=</span> <span class="n">cam</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">cam</span><span class="p">)</span>
  <span class="n">cam</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">(</span><span class="mi">255</span> <span class="o">*</span> <span class="n">cam</span><span class="p">)</span>
  <span class="n">cam</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="n">cam</span><span class="p">,</span> <span class="n">upsample_size</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">cam</span>
</code></pre></div></div>
<p>By implementing this code, you can get a heatmap of the image you want to see.</p>

<p><strong>Settings</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dataset</span> <span class="o">=</span> <span class="s">'CIFAR10'</span>
<span class="n">img_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">epoch</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
</code></pre></div></div>
<p>These are the settings for my experiment. The number of epochs can vary depending on the condition.</p>

<p>Here are some results.</p>

<p><img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/bird1.jpg?raw=true" alt="test1" /> <img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/bird5.png?raw=true" alt="cam1" /><br />
<img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/horse1.jpg?raw=true" alt="test2" /> <img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/horse5-1.png?raw=true" alt="cam2" /><br />
<img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/ship1.jpeg?raw=true" alt="test3" /> <img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/ship40.png?raw=true" alt="cam3" /></p>

<p>The results show that class activation maps are highlighting the discriminative object parts detected by the CNN.</p>

<p>While implementing, it would be a bit hard to see the results because the size of CIFAR10 images is 32x32.<br />
I would recommend you to train the model with another dataset if possible, or just take a pretrained model like GoogLeNet for better results.
If you use the pretrained model, you don’t need to train it anymore. You can use it as it is.</p>

<p>And this is the results of comparison between 5-epoch-experiment and 50-epoch-experiment.
I captured the model every 10 epochs during training and observed how the results change.</p>

<p>      5 Epoch            50 Epoch<br />
<img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/dog5-1.png?raw=true" alt="epoch5-1" title="5 epochs" /> <img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/dog50-1.png?raw=true" alt="epoch50-1" title="50 epochs" /><br />
<img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/dog5-2.png?raw=true" alt="epoch5-2" title="5 epochs" /> <img src="https://github.com/froggydisk/froggydisk.github.io/blob/master/assets/img/dog50-2.png?raw=true" alt="epoch50-2" title="50 epochs" /></p>

<p>As expected, you can find the model concentrates on the details of the image as the number of epochs increases.
Consequently, we can say global average pooling CNNs can perform accurate object localization.</p>

<h2 id="reference">Reference</h2>

<p><a href="https://kangbk0120.github.io/articles/2018-02/cam">https://kangbk0120.github.io/articles/2018-02/cam</a></p>]]></content><author><name>./workspace</name><email>sewoomkim93@gmail.com</email></author><category term="Assignment" /><category term="Pytorch" /><summary type="html"><![CDATA[Learning Deep Features for Discriminative Localization]]></summary></entry><entry><title type="html">Top1 Accuracy와 Top5 Accuracy 이해하기</title><link href="http://localhost:4000/2nd-post/" rel="alternate" type="text/html" title="Top1 Accuracy와 Top5 Accuracy 이해하기" /><published>2020-06-12T00:00:00+07:00</published><updated>2020-06-12T00:00:00+07:00</updated><id>http://localhost:4000/2nd-post</id><content type="html" xml:base="http://localhost:4000/2nd-post/"><![CDATA[<p><br /></p>
<p align="justify">
딥러닝 관련 논문을 읽다보면 Top1, Top5, Top10과 같은 말들을 자주 볼 수 있습니다. <br />
대충 감이 오기는 하지만 정확히 무엇을 의미하는 것일까요?  <br />
Top1 뒤에는 Accuracy가 오는 경우와 Loss가 오는 경우 두 가지가 있습니다.  <br />
Top1 Accuracy의 경우에는 가장 가까운 클래스를 예측했을 때 정답이었던 경우의 전체 경우의 수에 대한 비율을 의미합니다.<br />
반대로 Top1 Loss의 경우에는 가장 가까운 클래스를 예측했을 때 정답이 아니었던 경우의 전체 경우의 수에 대한 비율을 의미합니다.  <br />
예를 들어 강아지 고양이 미어캣 개구리 너구리를 구분하는 분류기가 있다고 합시다.<br />
위의 다섯 동물 중 어느 한 동물의 이미지를 분류기에 집어넣었더니 분류기의 마지막 층으로부터 Softmax 함수를 지나서 나온 결과가 (0.1, 0.15, 0.3, 0.4, 0.05)였다고 합시다.     <br />
벡터의 각 요소가 (강아지, 고양이, 미어캣, 개구리, 너구리)순으로 각각의 클래스로 분류될 확률이라고 한다면 이미지가 분류될 클래스 중에 가장 확률이 높은 것은 개구리가 됩니다.  <br />
같은 방법으로 두번째로 확률이 높은 클래스는 미어캣이 되겠네요.<br />
이미지가 분류될 확률이 높은 순으로 클래스를 나열하면 개구리 -&gt; 미어캣 -&gt; 고양이 -&gt; 강아지 -&gt; 너구리 순이 됩니다.  <br />
만약에 실제로 넣은 이미지가 개구리였다고 했을 때 Top1 Accuracy는 1이 되고 Top1 Loss는 0이 되겠네요.  <br />
예상하셨겠지만 Top5 Accuracy는 가장 가까운 클래스를 다섯 개 뽑았을 때 그 안에 정답이 존재한 경우의 비율을 의미합니다.<br />
참고로 분류기가 5개의 클래스 밖에 구분하지 못하므로 여기서는 Top5 Accuracy는 의미가 없다고 할 수 있습니다. <br />
입력 이미지가 위의 다섯 종류의 동물 중 하나라고 했을 때, 분류될 수 있는 클래스가 다섯개 뿐이므로 무조건 정답이 Top5 안에는 들어있게 되기 때문이죠.<br />
혹여나 입력 이미지가 위의 다섯 종류에 국한되어 있지 않는다면 이야기는 달라집니다만 여기서는 고려하지 않겠습니다.  <br />
보통 Top1 Accuracy + Top1 Loss = 1 이 되므로 Accuracy든 Loss든 결국에는 같은 이야기를 하고 있다고 생각하시면 됩니다. 
</p>]]></content><author><name>./workspace</name><email>sewoomkim93@gmail.com</email></author><category term="Blog" /><category term="Paper" /><summary type="html"><![CDATA[논문에 자주 등장하는 Top1, Top5의 의미는?]]></summary></entry><entry><title type="html">github.io 블로그 시작하기</title><link href="http://localhost:4000/1st-post/" rel="alternate" type="text/html" title="github.io 블로그 시작하기" /><published>2020-05-02T00:00:00+07:00</published><updated>2019-05-04T20:06:00+07:00</updated><id>http://localhost:4000/1st-post</id><content type="html" xml:base="http://localhost:4000/1st-post/"><![CDATA[<p>GitHub Blog 서비스인 github.io 블로그를 시작하기로 했다.</p>

<p>YFM에서 정의한 제목을 이중 괄호 구문으로 본문에 추가할 수 있다.
이 글의 제목은 “github.io 블로그 시작하기”이고
마지막으로 수정된 시간은 “2019-05-04 08:06:00 -0500”이다.</p>]]></content><author><name>./workspace</name><email>sewoomkim93@gmail.com</email></author><category term="Blog" /><category term="Blog" /><summary type="html"><![CDATA[GitHub Blog 서비스인 github.io 블로그 시작하기로 했다.]]></summary></entry></feed>